# FirstArtificial

Сегодня вечером, при изучении учебника Дж.Граса по науке о данных, я наткнулся на то, что нейросеть чисто на пресептронах не в состоянии построить обычный логический вентиль
XOR, я немного выпал с этого и пошёл шерстить нет как это можно реализовать с помощью библиотек по DeepLearning, которые я знаю.Я решил сравнить свой родной PyTorch с 
keras (не в плане функционала, а в плане удобства и понятности для таких как я новокеков в мире DataScience, ML & AI).Выводы неутешительные, pyTorch оказался для меня как C++ по сравнению с .lua & Python, насчёт полной функциональности ничего не знаю насчёт keras, но вроде как эта штучка часть от TensorFlow. 

Тут я попросил великий разум предоставить мне немного статистики об этих двух библиотеках в графике : 

![image](https://user-images.githubusercontent.com/39564937/225706621-6a31962b-6791-4329-be14-4612583a3e64.png)


Сам график:

![image](https://user-images.githubusercontent.com/39564937/225706713-de4a0f86-7e1d-43f2-91e4-de1849c3808d.png)

Выводы:
на вкус и цвет товарищей нет


Захотел я накидать блок-схему работы алгоритма с помощью либ на пайтоне и обнаружил такую странную вещь:
НЕТ НОРМАЛЬНЫХ ЛИБ ДЛЯ РИСОВАНИЯ БЛОК-СХЕМ С ПОМОЩЬЮ КОДА ОНИ ПРОСТО НЕ СУЩЕСТВУЮТ ЛИБО ВСЕ КРИВЫЕ И НАДО ЛЕПИТЬ КОСТЫЛИ

В итоге всё завершилось обычным ASCII:

![image](https://user-images.githubusercontent.com/39564937/225715252-a8b1cfc0-3699-41dc-bed0-8b5bad7ee445.png)

